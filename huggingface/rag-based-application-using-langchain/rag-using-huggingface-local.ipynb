{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf7a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "access_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73ec85",
   "metadata": {},
   "source": [
    "### **Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee76cefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"data/attention-is-all-you-need.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "258000f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brai\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cee9b6",
   "metadata": {},
   "source": [
    "### **Splitting Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd0f53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a565f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec184f25",
   "metadata": {},
   "source": [
    "### **Store in a vector database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d58569",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select an embeddings model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30dbe0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define vector store\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a8eff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0785f3ae-7ee1-433b-8143-700016edf7fb', '90bf0e96-e6d9-4350-9b62-213994ccefe1', '2f4d482e-6c28-429b-a90b-fa48372d9733']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c4927",
   "metadata": {},
   "source": [
    "### **Download our model Locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee274824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\Runs_llm_locally\\huggingface\\rag-based-application-using-langchain\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SVA_Delta\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-Coder-0.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "#model_name=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "#model_name=\"microsoft/Phi-3-mini-4k-instruct\"  # too much time it takes to response\n",
    "#model_name = \"openai-community/gpt2\"   # Give error\n",
    "model_name = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93091504",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(f\"tokenizer/{model_name}\")\n",
    "model.save_pretrained(f\"models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b4994bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI developed by Alibaba Cloud.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75f3bd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9727d0b3",
   "metadata": {},
   "source": [
    "### **Create a RAG pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "254ef473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_classic.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0110a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",  # or \"mmr\"\n",
    "    search_kwargs={\"k\": 3}     # number of docs to retrieve per query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9cbf616",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    You are an intelligent and helpful AI assistant designed to provide accurate, reliable, and natural responses.\n",
    "\n",
    "    Your primary goal is to answer questions based on the provided context.(Do not mention context in response)\n",
    "    If the context is relevant to the question, use it to produce a clear, well-structured answer.\n",
    "    If the context does not contain the answer, rely on your general knowledge to respond accurately and naturally.\n",
    "    If neither context nor general knowledge provides a valid answer, say \"I don't know.\"\n",
    "\n",
    "    You should:\n",
    "    - Use plain text only (no bullet points, tables, or markdown formatting).\n",
    "    - Respond naturally, like a human in conversation (e.g., if greeted, respond casually).\n",
    "    - Provide concise answers when appropriate, but be detailed when necessary.\n",
    "    - Avoid hallucination and never make up facts.\n",
    "    - Be able to answer general world questions as well (e.g., “What is the capital of Bangladesh?”).\n",
    "    - When summarizing or explaining content from the PDF, keep it precise and clear.\n",
    "    - When unsure, politely express uncertainty.\n",
    "    - You should act like an human agent, where after provide answer you should ask user that if anything more they want to know or not on that context. Not need to said this as same as this, just ask on similar type of things in a polite way.\n",
    "    - Your tone should be soft.\n",
    "\n",
    "    Context (from documents):\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9df0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,           \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt} \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef37a2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are an intelligent and helpful AI assistant designed to provide accurate, reliable, and natural responses.\n",
      "\n",
      "    Your primary goal is to answer questions based on the provided context.(Do not mention context in response)\n",
      "    If the context is relevant to the question, use it to produce a clear, well-structured answer.\n",
      "    If the context does not contain the answer, rely on your general knowledge to respond accurately and naturally.\n",
      "    If neither context nor general knowledge provides a valid answer, say \"I don't know.\"\n",
      "\n",
      "    You should:\n",
      "    - Use plain text only (no bullet points, tables, or markdown formatting).\n",
      "    - Respond naturally, like a human in conversation (e.g., if greeted, respond casually).\n",
      "    - Provide concise answers when appropriate, but be detailed when necessary.\n",
      "    - Avoid hallucination and never make up facts.\n",
      "    - Be able to answer general world questions as well (e.g., “What is the capital of Bangladesh?”).\n",
      "    - When summarizing or explaining content from the PDF, keep it precise and clear.\n",
      "    - When unsure, politely express uncertainty.\n",
      "    - You should act like an human agent, where after provide answer you should ask user that if anything more they want to know or not on that context. Not need to said this as same as this, just ask on similar type of things in a polite way.\n",
      "    - Your tone should be soft.\n",
      "\n",
      "    Context (from documents):\n",
      "    2017.\n",
      "[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "In International Conference on Learning Representations, 2017.\n",
      "[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722, 2017.\n",
      "[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130, 2017.\n",
      "[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\n",
      "Information Processing Systems, (NIPS), 2016.\n",
      "10\n",
      "\n",
      "2017.\n",
      "[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "In International Conference on Learning Representations, 2017.\n",
      "[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722, 2017.\n",
      "[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130, 2017.\n",
      "[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\n",
      "Information Processing Systems, (NIPS), 2016.\n",
      "10\n",
      "\n",
      "2017.\n",
      "[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "In International Conference on Learning Representations, 2017.\n",
      "[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722, 2017.\n",
      "[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130, 2017.\n",
      "[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\n",
      "Information Processing Systems, (NIPS), 2016.\n",
      "10\n",
      "\n",
      "    Question:\n",
      "    What is this paper about?\n",
      "\n",
      "    Answer:\n",
      "     This paper presents a structured attention network, which is a type of neural network that can capture long-range dependencies between input data. The authors use a factorization trick to enhance the performance of the network by reducing the dimensionality of the input data. The authors also study the effect of different types of attention mechanisms on the performance of the network. \n",
      "\n",
      "This paper is about structured attention networks, which is a type of neural network that can capture long-range dependencies between input data. The authors use a factorization trick to enhance the performance of the network by reducing the dimensionality of the input data. The authors also study the effect of different types of attention mechanisms on the performance of the network.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is this paper about?\"\n",
    "answer = qa_chain.run(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
